{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61acd0f0-1a4e-4069-9320-726872408c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820ab40-dfb6-4c7b-8c76-ca830e00da77",
   "metadata": {},
   "source": [
    "# Numpy vs. Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f402f-edb8-4b20-95e7-95bae3714f34",
   "metadata": {},
   "source": [
    "* Numpy **arrays** and pytorch **tensors** can be created in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f46cd458-bbb9-44a8-a672-9c12bb308283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "730f622f-d21a-4073-84ee-41117033a9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed43e4eb-270a-4509-b584-b24d9b94887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba4c71-663d-4791-be93-2a2fb1bccf0d",
   "metadata": {},
   "source": [
    "* They can be resized in similar ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91f66aa7-a237-4151-b98e-bb08d9a27ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23],\n",
       "         [24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(48).reshape(1,3,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "364fdddf-97d2-42fe-a16c-bbf82c471891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11],\n",
       "          [12, 13, 14, 15]],\n",
       "\n",
       "         [[16, 17, 18, 19],\n",
       "          [20, 21, 22, 23],\n",
       "          [24, 25, 26, 27],\n",
       "          [28, 29, 30, 31]],\n",
       "\n",
       "         [[32, 33, 34, 35],\n",
       "          [36, 37, 38, 39],\n",
       "          [40, 41, 42, 43],\n",
       "          [44, 45, 46, 47]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(48).reshape(1,3,4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739c336-9d1e-47f7-b0b8-99e37cfefdf3",
   "metadata": {},
   "source": [
    "* Similar addition and multiplication rules apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47579ef8-5c78-4216-a6f3-083571a9b923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((1,1,1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dcfe77a3-20b8-4e90-90bd-283fb433ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 3,  2,  6,  8],\n",
       "         [ 7,  6, 10, 12],\n",
       "         [11, 10, 14, 16],\n",
       "         [15, 14, 18, 20]],\n",
       "\n",
       "        [[19, 18, 22, 24],\n",
       "         [23, 22, 26, 28],\n",
       "         [27, 26, 30, 32],\n",
       "         [31, 30, 34, 36]],\n",
       "\n",
       "        [[35, 34, 38, 40],\n",
       "         [39, 38, 42, 44],\n",
       "         [43, 42, 46, 48],\n",
       "         [47, 46, 50, 52]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(48).reshape(1,3,4,4) + np.array([3,1,4,5]).reshape(1,1,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7dca407e-2aa3-49a5-be7a-cc311702d08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3,  2,  6,  8],\n",
       "          [ 7,  6, 10, 12],\n",
       "          [11, 10, 14, 16],\n",
       "          [15, 14, 18, 20]],\n",
       "\n",
       "         [[19, 18, 22, 24],\n",
       "          [23, 22, 26, 28],\n",
       "          [27, 26, 30, 32],\n",
       "          [31, 30, 34, 36]],\n",
       "\n",
       "         [[35, 34, 38, 40],\n",
       "          [39, 38, 42, 44],\n",
       "          [43, 42, 46, 48],\n",
       "          [47, 46, 50, 52]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(48).reshape(1,3,4,4) + torch.tensor([3,1,4,5]).reshape(1,1,1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfbbba-42fe-4d6d-9b5f-095c56ecdc84",
   "metadata": {},
   "source": [
    "Other miscellaneous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "93792b88-47e3-4cb5-82ce-fe7afe83f0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3,  2,  6,  8],\n",
       "         [ 7,  6, 10, 12],\n",
       "         [11, 10, 14, 16],\n",
       "         [15, 14, 18, 20]],\n",
       "\n",
       "        [[19, 18, 22, 24],\n",
       "         [23, 22, 26, 28],\n",
       "         [27, 26, 30, 32],\n",
       "         [31, 30, 34, 36]],\n",
       "\n",
       "        [[35, 34, 38, 40],\n",
       "         [39, 38, 42, 44],\n",
       "         [43, 42, 46, 48],\n",
       "         [47, 46, 50, 52]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(48).reshape(3,4,4) + torch.tensor([3,1,4,5]).reshape(1,1,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825dd39-a839-4050-b884-ab88e90b1191",
   "metadata": {},
   "source": [
    "Taking maxima across different dimensions\n",
    "\n",
    "* For the 3D array above `dim=0` is like **depth**, and `dim=1` and `dim=2` correspond to **rows** and **columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3dcbe270-0027-4246-a17f-ca4a69ec400d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[15, 14, 18, 20],\n",
       "        [31, 30, 34, 36],\n",
       "        [47, 46, 50, 52]]),\n",
       "indices=tensor([[3, 3, 3, 3],\n",
       "        [3, 3, 3, 3],\n",
       "        [3, 3, 3, 3]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2d157-d8ff-4c6a-8ed5-64edb0b57471",
   "metadata": {},
   "source": [
    "**Pytorch** starts to really differ from **numpy** in terms of automatically computing gradients of operations\n",
    "\n",
    "$$y = \\sum_i x_i^3$$\n",
    "\n",
    "has a gradient\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = 3x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "49edfec1-092f-4c04-a1c1-298bf1e861e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(917., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad=True)\n",
    "y = x.pow(3).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "85a51b1c-fb36-48c3-8364-5c6e292de13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d84d76b4-c504-4dbe-94b3-3b846891c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c57eaa-419f-4b92-974b-2a96918eab1f",
   "metadata": {},
   "source": [
    "This is useful for computing gradients for neural network operations (later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b4c4f-0d94-45f3-a4a3-543cbbf128e5",
   "metadata": {},
   "source": [
    "# Dealing With Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef036e0-20b0-4155-919d-b7632e409d7a",
   "metadata": {},
   "source": [
    "Image tensors have the shape $[N,C,H,W]$ where\n",
    "\n",
    "* $N$ is the batch size\n",
    "* $C$ is the number of channels\n",
    "* $H$ and $W$ are the height and width of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2a686f03-de89-47ec-943c-09e61b7c2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(5, 3, 30, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271b5c2-5c21-434e-a120-c9af7a0bce48",
   "metadata": {},
   "source": [
    "Convolutional kernels `nn.Conv2d(C1,C2)` map an image tensor from the shape $[N,C_1,...]$ to $[N,C_2,...]$ and also modify the height and width. As such, the first rgument of `nn.Conv2d` needs to match the second dimension of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1f9bdf72-0d6a-49b8-817f-f5dcf0b47392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 7, 30, 50])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = nn.Conv2d(3, 7, kernel_size=3, stride=1, padding=1)\n",
    "C(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8810c6-56c5-4a93-a550-474d0c49189b",
   "metadata": {},
   "source": [
    "# Testing Out Things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e62f28-ddc1-4eaf-baed-09c343af0c74",
   "metadata": {},
   "source": [
    "3 images, 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b8ca1bac-8333-4051-8ce4-834538a88076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3397)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.rand(3*10*192*192).reshape(3,10,192,192)\n",
    "labels = (10*torch.rand(3*192*192).reshape(3,192,192)).long()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss(pred,labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
