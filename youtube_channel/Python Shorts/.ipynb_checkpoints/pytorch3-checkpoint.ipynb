{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b766e8a3-ea3c-4e69-8e6a-0cfd9ab9d556",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afd9f8-85fe-40f7-8913-5d5933dd081b",
   "metadata": {},
   "source": [
    "# Machine Learning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ace27-b0b5-4a50-80cd-b52705d62ae6",
   "metadata": {},
   "source": [
    "Suppose one has data that consists of an independent vector and a dependent vector $x_i$ and $y_i$ ($i$ is the ith value in the data set). For example:\n",
    "\n",
    "* $x_i$ is the height of the $i$th person, and $y_i$ is their weight (predict weight using height)\n",
    "* $x_i$ is a picture of a handwritten digit, and $y_i$ is the digit itself (predict numbers from hand written numbers)\n",
    "* $x_i$ is a CT scan of a patient, and $y_i$ are the corresponding pixels corresponding to tumours (my research)\n",
    "\n",
    "The goal of a neural network is as follows. Define a function $f$ that depends on parameters $a$ that makes predictions\n",
    "\n",
    "$$\\hat{y_i} =f(x_i;a)$$\n",
    "\n",
    "One wants to make $\\hat{y_i}$ (the predictions) and $y_i$ (the true values) as close as possible by modifying the values of $a$. What does as close as possible mean? This depends on the task. In general, one defines a similarity function (or **Loss** function) $L(y,\\hat{y})$. The more similar all the $y_i$s and $\\hat{y_i}$s are, the smaller $L$ should be. For example 1 above, this could be as simple as \n",
    "\n",
    "$$L(y,\\hat{y}) = \\sum_i(y_i-\\hat{y_i})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169d253-e893-47a3-9567-284345c667ce",
   "metadata": {},
   "source": [
    "# Last Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c646f3c-2d0c-4c1e-9e94-0d097c7c666d",
   "metadata": {},
   "source": [
    "Given 4 data points $(x_i, y_i)$ and wanted to find a function $f$ such that $f(x_i)=y_i$. Since each $x_i$ was a vector of length $2$, we chose the function\n",
    "\n",
    "$$f(x) = A_2 A_1 x$$\n",
    "\n",
    "where $A_1$ is a $8 \\times 2$ matrix and $A_2$ is a $1 \\times 8$ matrix. This means there were $16+8$ free parameters. This simple function did not do a very good job of making $f(x_i)=y_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c9079-5b07-49d4-af8e-6ecc0d997b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "\n",
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,8,bias=False)\n",
    "        self.Matrix2 = nn.Linear(8,1,bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.Matrix1(x)\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()\n",
    "    \n",
    "f = MyNeuralNet()\n",
    "opt = SGD(f.parameters(), lr=0.001)\n",
    "L = nn.MSELoss()\n",
    "\n",
    "# Train model\n",
    "losses = []\n",
    "for _ in range(50):\n",
    "    opt.zero_grad() # flush previous epoch's gradient\n",
    "    loss_value = L(f(x), y) #compute loss\n",
    "    loss_value.backward() # compute gradient\n",
    "    opt.step() # Perform iteration using gradient above\n",
    "    losses.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac8d75-1cf1-4b20-b7f9-75a19e27fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ea740-d0bb-4144-a8f4-51f17fa85356",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453716f-36c9-4104-bdb8-de852cc4018b",
   "metadata": {},
   "source": [
    "# This Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6cc50c-818d-4845-a780-1e9d29138cd9",
   "metadata": {},
   "source": [
    "As it turns out, our previous model was not very food at all. In fact, although there were 24 parameters in the two matrices, there was technically only **two** independent parameters. This is because\n",
    "\n",
    "$$A_2 A_1 = B$$\n",
    "\n",
    "where $B$ is a $2 \\times 1$ matrix. So really our function was $f(x) = Bx$\n",
    "\n",
    "## How can we use this simplicity of linear algebra but have advanced models?\n",
    "\n",
    "**The Crux of Machine Learning**: This lies in so-called activation functions, which add ever-so-slight non-linearities to a sequence of matrix transformations. Instead of the transformation\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f(x) = A_2 A_1 x$$\n",
    "\n",
    "consider instead\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_2(x) = A_2 R(A_1 x)$$\n",
    "\n",
    "where $R$ is an element-wise operator defined by\n",
    "\n",
    "$$R(x) = \\begin{cases}x & x>0 \\\\ 0&  x \\leq 0 \\end{cases}$$\n",
    "\n",
    "So $R$ is the identity function if $x>0$ but sets values equal to zero if $x$ is less than zero. This is **so-close** to being a linear operator, but it is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da39d3-262b-4f6f-8df3-db4e774ccf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[4,6,2,-1,6,2,5],[1,6,2,-6,5,-3,5]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f1ea9-7122-4a84-b704-a11a4ef8b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = nn.ReLU()\n",
    "R(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2abac-c7bc-4fa5-be73-120b081afb42",
   "metadata": {},
   "source": [
    "Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04494322-66b9-466f-9ec9-ffb9f7658663",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-3,3,100)\n",
    "y = R(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e00df6-1678-45c3-ad03-8c6db2ae7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy(), y.numpy())\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ebbed-9bac-4f83-866f-0c1134729b44",
   "metadata": {},
   "source": [
    "How much better does our model do with this simple adjustment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1232d-ecc6-49c5-b827-8715a63c6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,8,bias=False)\n",
    "        self.Matrix2 = nn.Linear(8,1,bias=False)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2caf3-84fd-4c56-90d5-3cc1ac16d633",
   "metadata": {},
   "source": [
    "Train model (lets write a function to do this, since we'll be doing it a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d0074-a1b5-48a5-a061-761b5380f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x,y,f, n_epochs=50):\n",
    "    opt = SGD(f.parameters(), lr=0.001)\n",
    "    L = nn.MSELoss()\n",
    "\n",
    "    # Train model\n",
    "    losses = []\n",
    "    for _ in range(n_epochs):\n",
    "        opt.zero_grad() # flush previous epoch's gradient\n",
    "        loss_value = L(f(x), y) #compute loss\n",
    "        loss_value.backward() # compute gradient\n",
    "        opt.step() # Perform iteration using gradient above\n",
    "        losses.append(loss_value.item())\n",
    "    return f, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec199b6-e8a5-41b0-b79e-5b57e4cf2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f2 = MyNeuralNet2()\n",
    "\n",
    "# Train model\n",
    "f2, losses2 = train_model(x,y,f2, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136eefb-8135-4cca-b11d-b54230b7219c",
   "metadata": {},
   "source": [
    "Now lets look at the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb25296-9055-4015-8c4c-fa703f876ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef4276-0202-4db0-a175-a6e3890ff3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2ccf9-5d48-4397-89de-694e8ec9af76",
   "metadata": {},
   "source": [
    "Slightly better. But the real advantage of this slight non-linearity is that we can make our matrices much larger. Lets make our matrices size $80 \\times 2$ and $1 \\times 80$. This **only** works because of our non-linearity function $R(x)$:\n",
    "\n",
    "* Without $R(x)$, we would just have $A_2 A_1 = B$ and so $f(x) = Bx$ where $B$ is still a $1 \\times 2$ matrix even though $A_2$ and $A_1$ are larger matrices. The non-linearity function $R(x)$, to some extent, makes all 240 parameters more independent from eachother.\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f_2(x) = A_2 R(A_1 x) \\hspace{8mm} \\text{$A_2$ is 1x8 and $A_1$ is 8x2}$$\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_3(x) = A_2 R(A_1 x)  \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218c675-2b5e-480a-80c1-a015094cd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,80, bias=False)\n",
    "        self.Matrix2 = nn.Linear(80,1, bias=False)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3639332-e890-4154-a5c7-148b22195b7a",
   "metadata": {},
   "source": [
    "Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a08204-e82b-4a18-8148-098f0459fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f3 = MyNeuralNet3()\n",
    "\n",
    "# Train model\n",
    "f3, losses3 = train_model(x,y,f3, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d683b12-cc1c-49f2-861f-fcf56d4cad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5969e-a58f-4bbf-9570-7f76a566ddd7",
   "metadata": {},
   "source": [
    "Closer, but still not exact. We can make our model better by introducing other parameters:\n",
    "\n",
    "$$f(x) = A_2 R(A_1x + b_1) + b_2$$\n",
    "\n",
    "where $b_1$ and $b_2$ are vectors added to each of the linear transformations.\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f_3(x) = A_2 R(A_1 x) \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_4(x) = A_2 R(A_1 x+b_1)+b_2  \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb086c4-eedc-47da-8bef-56e5e7f9fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,80)\n",
    "        self.Matrix2 = nn.Linear(80,1)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a77b9-3b43-43b7-b37c-8af541d55018",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f4 = MyNeuralNet4()\n",
    "\n",
    "# Train model\n",
    "f4, losses4 = train_model(x,y,f4, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75560f6-fbc5-4f35-b8af-d28370adaa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "f4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa448b68-7e32-4ffb-85c9-9dceba98676c",
   "metadata": {},
   "source": [
    "Better, but its still not getting us that close to $y$, however. What if we add another matrix in the middle?\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f_4(x) = A_2 R(A_1 x+b_1)+b_2  \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_5(x) = A_3 R(A_2 R(A_1 x+b_1)+b_2)  \\hspace{8mm} \\text{$A_3$ is 1x80 and $A_2$ is 80x80 and $A_1$ is 80x2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a8bb3-96a5-4510-b50c-6af860270234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,80)\n",
    "        self.Matrix2 = nn.Linear(80,80)\n",
    "        self.Matrix3 = nn.Linear(80,1)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.R(self.Matrix2(x))\n",
    "        x = self.Matrix3(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f5 = MyNeuralNet5()\n",
    "\n",
    "# Train model\n",
    "f5, losses5 = train_model(x,y,f5, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ea596-be20-4f53-826d-eaf41150498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9932ef-d55f-4b2c-a933-bd72fb3e2d5b",
   "metadata": {},
   "source": [
    "Its predicting $y$ almost exactly (albeit by overfitting, no doubt, but the message here is that the model has the potential to fit to these arbitrary data points, through a sequence of **linear** transofmrations followed by slightly non-linear )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec711b12-f331-4150-89ca-b5b90456583e",
   "metadata": {},
   "source": [
    "# \"The Neural Network\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e899fb5-8fec-48cc-a711-81da792324fb",
   "metadata": {},
   "source": [
    "A general \"sequential\" neural network can be expressed as\n",
    "\n",
    "$$f(x) = \\underset{i=1}{\\overset{n}{\\Huge{\\kappa}}} R_i(A_ix+b_i)$$\n",
    "\n",
    "where $\\underset{i=1}{\\overset{n}{\\Huge{\\kappa}}}f_i(x) = f_n \\circ f_{n-1} ... \\circ f_1(x)$ and the $A_i$ are matrices and the $b_i$ are bias vectors. Typically the $R_i$ are the same for all the layers (typically ReLU) **except** for the last layer, where $R_i$ is just is just the identity function\n",
    "\n",
    "* **Note**: In clever architectures, like convolutional neural networks, the $A_i$'s become sparse matrices (most of there parameters are fixed to equal zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317894c4-0fbb-4baf-abdf-b576cb9ed68a",
   "metadata": {},
   "source": [
    "# Next Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f8132-e249-4659-971b-c0c16c212215",
   "metadata": {},
   "source": [
    "Training on some real data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
